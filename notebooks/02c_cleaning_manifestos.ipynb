{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e9cc08",
   "metadata": {},
   "source": [
    "Context: This script constitutes the core logic of the 02c_cleaning_manifestos.ipynb notebook. It addresses the need to integrate party manifestos (platforms) into the analysis, complementing the previously cleaned candidate speeches to form a comprehensive textual corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4aab5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unrecognized filename format for: 2020_Republicans.txt\n",
      "Total number of texts in the corpus: 27\n",
      "   year       party candidate  \\\n",
      "0  2024    Democrat    Harris   \n",
      "1  2024  Republican     Trump   \n",
      "2  2020    Democrat     Biden   \n",
      "3  2020  Republican     Trump   \n",
      "4  2016    Democrat   Clinton   \n",
      "\n",
      "                                                text  \n",
      "0  Good evening! Kamala! Kamala! Kamala! Californ...  \n",
      "1  Thank you very much. Thank you very, very much...  \n",
      "2  Good evening. Ella Baker, a giant of the civil...  \n",
      "3  Thank you very much. Thank you very much. Than...  \n",
      "4  Thank you all very, very much! Thank you for t...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: DEFINE SPECIFIC CLEANING FUNCTION\n",
    "# ==========================================\n",
    "def clean_platform_text(text):\n",
    "    \"\"\"\n",
    "    Applies text preprocessing specifically tailored for political manifestos \n",
    "    (often derived from PDF conversions), removing legal disclaimers and formatting artifacts.\n",
    "    \"\"\"\n",
    "    # Remove standard campaign legal disclaimers (e.g., \"Paid for by...\").\n",
    "    text = re.sub(r'Paid for by.*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Not Authorized By.*', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove isolated page numbers often found in PDF text extracts.\n",
    "    # Pattern 1: Digits appearing alone on a new line.\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', ' ', text) \n",
    "    # Pattern 2: Isolated digits surrounded by whitespace within the text.\n",
    "    text = re.sub(r'\\s\\d+\\s', ' ', text)       \n",
    "    \n",
    "    # Note: Repeated headers/footers (e.g., \"Republican Platform 2012\") can be added here\n",
    "    # if identified as recurring patterns.\n",
    "    \n",
    "    # Whitespace Normalization (PDF artifact correction): \n",
    "    # Replace newline characters with spaces to reconstruct broken sentences.\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Collapse multiple whitespace characters into a single space.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: FILE PROCESSING LOOP\n",
    "# ==========================================\n",
    "def process_platforms(folder_path):\n",
    "    \"\"\"\n",
    "    Iterates through text files in the specified directory, extracts metadata \n",
    "    from filenames, and applies the cleaning function.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Metadata Extraction: Parse year and party from the filename.\n",
    "            # Assumes format: \"YYYY-Party.txt\" (e.g., \"2004-Republicans.txt\")\n",
    "            try:\n",
    "                year_str, party_raw = filename.replace('.txt', '').split('-')\n",
    "                year = int(year_str)\n",
    "                # Standardize party names based on filename content\n",
    "                party = \"Republican\" if \"Republicans\" in party_raw else \"Democrat\"\n",
    "            except:\n",
    "                print(f\"Warning: Unrecognized filename format for: {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Read and clean the file content\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                raw_text = f.read()\n",
    "                cleaned_text = clean_platform_text(raw_text)\n",
    "            \n",
    "            # Assign a generic candidate name for manifestos.\n",
    "            candidate_name = \"Party Platform\" \n",
    "            \n",
    "            data.append({\n",
    "                'year': year,\n",
    "                'party': party,\n",
    "                'candidate': candidate_name, \n",
    "                'text': cleaned_text\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: EXECUTION AND MERGING\n",
    "# ==========================================\n",
    "\n",
    "# Define the directory path containing the raw manifesto text files.\n",
    "folder_path = '/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/manifestos' \n",
    "\n",
    "# Generate the manifestos DataFrame.\n",
    "df_platforms = process_platforms(folder_path)\n",
    "\n",
    "# Load the previously processed speeches dataset.\n",
    "df_speeches = pd.read_csv('/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/processed/president_speeches_clean.csv')\n",
    "\n",
    "# Standardize column selection to ensure schema consistency before merging.\n",
    "df_speeches = df_speeches[['year', 'party', 'candidate', 'text']]\n",
    "df_platforms = df_platforms[['year', 'party', 'candidate', 'text']]\n",
    "\n",
    "# Data Integration: Vertically concatenate speeches and manifestos.\n",
    "df_nlp_final = pd.concat([df_speeches, df_platforms], ignore_index=True)\n",
    "\n",
    "# Validation\n",
    "print(f\"Total number of texts in the corpus: {len(df_nlp_final)}\")\n",
    "print(df_nlp_final.head())\n",
    "\n",
    "# Export the unified NLP database.\n",
    "df_nlp_final.to_csv('nlp_database_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d6dd0",
   "metadata": {},
   "source": [
    "Context: This segment serves as the Quality Assurance (QA) phase within the 02c_cleaning_manifestos.ipynb notebook. Before proceeding to descriptive analysis or modeling, it is imperative to validate the structural integrity and content richness of the unified NLP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dd516a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "DATA CONTENT VERIFICATION\n",
      "------------------------------\n",
      "\n",
      "Document count per year:\n",
      "year\n",
      "2000    4\n",
      "2004    4\n",
      "2008    4\n",
      "2012    4\n",
      "2016    4\n",
      "2020    3\n",
      "2024    4\n",
      "dtype: int64\n",
      "\n",
      "Overview of the first 15 documents (Year, Party, Candidate, Text Length):\n",
      "    year       party       candidate  text_length\n",
      "0   2024    Democrat          Harris        20974\n",
      "20  2024    Democrat  Party Platform       270996\n",
      "1   2024  Republican           Trump        66964\n",
      "22  2024  Republican  Party Platform        39275\n",
      "2   2020    Democrat           Biden        17591\n",
      "19  2020    Democrat  Party Platform       287421\n",
      "3   2020  Republican           Trump        40805\n",
      "4   2016    Democrat         Clinton        29502\n",
      "15  2016    Democrat  Party Platform       182915\n",
      "5   2016  Republican           Trump        29128\n",
      "23  2016  Republican  Party Platform       240697\n",
      "6   2012    Democrat           Obama        25330\n",
      "14  2012    Democrat  Party Platform       169538\n",
      "7   2012  Republican          Romney        22519\n",
      "16  2012  Republican  Party Platform       211417\n",
      "\n",
      "Focus on Party Platforms (Manifestos):\n",
      "    year       party  text_length\n",
      "20  2024    Democrat       270996\n",
      "22  2024  Republican        39275\n",
      "19  2020    Democrat       287421\n",
      "15  2016    Democrat       182915\n",
      "23  2016  Republican       240697\n",
      "14  2012    Democrat       169538\n",
      "16  2012  Republican       211417\n",
      "17  2008    Democrat       174297\n",
      "24  2008  Republican       166851\n",
      "26  2004    Democrat       118102\n",
      "18  2004  Republican       267639\n",
      "25  2000    Democrat       159114\n",
      "21  2000  Republican       224064\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 1: DATA SORTING\n",
    "# ==========================================\n",
    "# Sort the dataframe chronologically (descending) and by party (alphabetical) \n",
    "# to ensure a structured view of the election cycles.\n",
    "df_nlp_final = df_nlp_final.sort_values(by=['year', 'party'], ascending=[False, True])\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: SANITY CHECK & VALIDATION\n",
    "# ==========================================\n",
    "print(\"-\" * 30)\n",
    "print(\"DATA CONTENT VERIFICATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# A. Document Completeness Check\n",
    "# Verify the distribution of documents across years. \n",
    "# Ideally, each election cycle should contain approx. 4 documents (2 Speeches + 2 Platforms).\n",
    "print(\"\\nDocument count per year:\")\n",
    "print(df_nlp_final.groupby('year').size())\n",
    "\n",
    "# B. Content Validity Assessment\n",
    "# Calculate the character count of each text entry.\n",
    "# This metric serves as a proxy to ensure that the cleaning process did not erroneously \n",
    "# delete valid content (i.e., checking for empty or near-empty strings).\n",
    "df_nlp_final['text_length'] = df_nlp_final['text'].apply(len)\n",
    "\n",
    "print(\"\\nOverview of the first 15 documents (Year, Party, Candidate, Text Length):\")\n",
    "print(df_nlp_final[['year', 'party', 'candidate', 'text_length']].head(15))\n",
    "\n",
    "# C. Platform-Specific Validation\n",
    "# Isolate the 'Party Platform' entries to verify they exhibit substantial length.\n",
    "# Manifestos are typically extensive documents (> 20,000 characters); \n",
    "# low counts here would indicate a PDF extraction failure.\n",
    "print(\"\\nFocus on Party Platforms (Manifestos):\")\n",
    "print(df_nlp_final[df_nlp_final['candidate'] == \"Party Platform\"][['year', 'party', 'text_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36d43d",
   "metadata": {},
   "source": [
    "Context: This segment of the 02c_cleaning_manifestos.ipynb notebook performs a targeted sanity check focusing on the 2012 election cycle. This step validates the structural integrity of the merged dataset (df_nlp_final) before proceeding to the descriptive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec86ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for the 2012 election cycle: 4\n",
      "    year       party       candidate  text_length\n",
      "6   2012    Democrat           Obama        25330\n",
      "14  2012    Democrat  Party Platform       169538\n",
      "7   2012  Republican          Romney        22519\n",
      "16  2012  Republican  Party Platform       211417\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 3: TARGETED VALIDATION (CASE STUDY: 2012)\n",
    "# ==========================================\n",
    "# Isolate the data for the 2012 election cycle to perform a granular spot check.\n",
    "# This specific year serves as a benchmark to ensure that both speeches and manifestos \n",
    "# were correctly merged and attributed.\n",
    "test_2012 = df_nlp_final[df_nlp_final['year'] == 2012]\n",
    "\n",
    "print(f\"Row count for the 2012 election cycle: {len(test_2012)}\")\n",
    "\n",
    "# Display key metadata and text metrics for the selected subset.\n",
    "# This allows for a visual comparison between the length of oral speeches (Candidates) \n",
    "# and written platforms (Party Platform).\n",
    "print(test_2012[['year', 'party', 'candidate', 'text_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95736807",
   "metadata": {},
   "source": [
    "Context: This script functions as the concluding operation of the data cleaning pipeline within 02c_cleaning_manifestos.ipynb. It is responsible for finalizing the structure of the text corpus before it is handed off to the descriptive and predictive modeling stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cf5d7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database sorted and index reset successfully.\n",
      "Preview of the first 10 rows (Should start with the 2024 election cycle):\n",
      "   year       party       candidate\n",
      "0  2024    Democrat          Harris\n",
      "1  2024    Democrat  Party Platform\n",
      "2  2024  Republican           Trump\n",
      "3  2024  Republican  Party Platform\n",
      "4  2020    Democrat           Biden\n",
      "5  2020    Democrat  Party Platform\n",
      "6  2020  Republican           Trump\n",
      "7  2016    Democrat         Clinton\n",
      "8  2016    Democrat  Party Platform\n",
      "9  2016  Republican           Trump\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 1: FINAL SORTING AND INDEX RESET\n",
    "# ==========================================\n",
    "\n",
    "# 1. Sort the dataset\n",
    "# The data is organized chronologically (Year: Descending) and then alphabetically by Party.\n",
    "# This structure facilitates the analysis of the most recent election cycles first.\n",
    "df_nlp_final = df_nlp_final.sort_values(by=['year', 'party'], ascending=[False, True])\n",
    "\n",
    "# 2. CRITICAL: Reset the Dataframe Index\n",
    "# After sorting, the indices are often disordered (e.g., 10, 5, 2). \n",
    "# Resetting ensures a sequential index (0, 1, 2...), which is essential for \n",
    "# consistent iteration and access using functions like .iloc[].\n",
    "# The 'drop=True' parameter prevents the old disordered index from being added as a new column.\n",
    "df_nlp_final = df_nlp_final.reset_index(drop=True)\n",
    "\n",
    "# 3. Save the Final Processed Dataset (Checkpoint)\n",
    "# Export the clean, sorted, and re-indexed dataframe to a CSV file.\n",
    "# This file serves as the stable input for the subsequent Analysis and Machine Learning notebooks.\n",
    "df_nlp_final.to_csv('nlp_database_sorted_clean.csv', index=False)\n",
    "\n",
    "print(\"Database sorted and index reset successfully.\")\n",
    "print(\"Preview of the first 10 rows (Should start with the 2024 election cycle):\")\n",
    "print(df_nlp_final[['year', 'party', 'candidate']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb4a91",
   "metadata": {},
   "source": [
    "Context: This script executes the final, most intensive data cleaning phase, designated as \"V8,\" within the 02c_cleaning_manifestos.ipynb notebook. It addresses persistent data quality issues identified during initial exploratory analysis, specifically targeting Optical Character Recognition (OCR) errors and irrelevant numerical noise found in the party manifestos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e503352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating V8 Intensive Cleaning Protocol...\n",
      "Process Complete. Cleaned database saved to: nlp_database_CLEAN_V8.csv\n",
      "\n",
      "Final Verification:\n",
      "SUCCESS: Artifact 'etek' has been eliminated.\n",
      "SUCCESS: Artifact 'nee' has been eliminated.\n",
      "SUCCESS: Artifact '6060' has been eliminated.\n",
      "SUCCESS: Artifact '0002' has been eliminated.\n",
      "SUCCESS: Artifact 'ees' has been eliminated.\n",
      "SUCCESS: Artifact 'MERICAN' has been eliminated.\n",
      "\n",
      "VERIFICATION SUCCESSFUL. The dataset is ready for Sentiment Analysis.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: DEFINE INTENSIVE CLEANING FUNCTION (V8)\n",
    "# ==========================================\n",
    "def cleaning_v8_scorched_earth(text):\n",
    "    \"\"\"\n",
    "    Applies a rigorous, 'scorched earth' cleaning protocol to eliminate persistent \n",
    "    OCR artifacts, irrelevant numerical data, and specific noise tokens identified \n",
    "    during manual inspection.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # --- 1. NUMERICAL CLEANING ---\n",
    "    # Strategy: Retain only years (19xx or 20xx). \n",
    "    # This removes irrelevant integers (e.g., page numbers, table data like \"6060\", \"0002\").\n",
    "    # Regex lookahead ensures the digits are not preceded by 19 or 20.\n",
    "    text = re.sub(r'\\b(?!19|20)\\d+\\b', ' ', text) \n",
    "    # Remove decimal numbers often found in statistical tables (e.g., \"0.0002\").\n",
    "    text = re.sub(r'\\b\\d+\\.\\d+\\b', ' ', text)     \n",
    "    \n",
    "    # --- 2. EXPLICIT BLACKLIST FILTERING ---\n",
    "    # Removal of specific nonsense tokens identified as recurrent OCR errors.\n",
    "    blacklist = [\n",
    "        \"etek\", \"nee\", \"ane\", \"EERE\", \"PR\", \"ees\", \"MERICAN\", \n",
    "        \"itecee\", \"cscce\", \"erenere\", \"ri\", \"DDLE\", \"eeaeva\"\n",
    "    ]\n",
    "    \n",
    "    for bad_word in blacklist:\n",
    "        # Regex removes the exact token (case-insensitive) to prevent partial matches within valid words.\n",
    "        text = re.sub(r'\\b' + re.escape(bad_word) + r'\\b', ' ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # --- 3. PUNCTUATION & SYMBOL CLEANING ---\n",
    "    # Collapse repeated punctuation marks (e.g., \": :..\") into a single space.\n",
    "    text = re.sub(r'[\\.:\\-,_]{2,}', ' ', text) \n",
    "    # Remove non-standard symbols (e.g., currency symbols, OCR artifacts), keeping only alphanumeric and basic punctuation.\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:\\'\\-?!]', ' ', text)\n",
    "\n",
    "    # --- 4. STRUCTURAL & PATTERN-BASED CLEANING ---\n",
    "    # Remove specific OCR noise patterns: words > 12 characters containing high-frequency noise letters.\n",
    "    text = re.sub(r'\\b[cesinrat_\\-]{12,}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Length Threshold: Remove strings exceeding 18 characters, which are statistically likely to be artifacts.\n",
    "    text = re.sub(r'\\b\\w{18,}\\b', ' ', text)\n",
    "    \n",
    "    # Fix Spaced Capitalization: Reconstruct words split by spaces (e.g., \"T A B L E\" -> \"TABLE\").\n",
    "    text = re.sub(r'\\b([A-Z])\\s+(?=[A-Z]\\b)', r'\\1', text)\n",
    "    \n",
    "    # Remove administrative headers and section titles.\n",
    "    text = re.sub(r'TABLE OF CONTENTS|CONTENTS|PREAMBLE|INTRODUCTION', ' ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Final Whitespace Normalization.\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "print(\"Initiating V8 Intensive Cleaning Protocol...\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: APPLY TRANSFORMATION\n",
    "# ==========================================\n",
    "df_nlp_final['text'] = df_nlp_final['text'].apply(cleaning_v8_scorched_earth)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: SORTING AND INDEXING\n",
    "# ==========================================\n",
    "# Ensure chronological and categorical order is maintained.\n",
    "df_nlp_final = df_nlp_final.sort_values(by=['year', 'party'], ascending=[False, True])\n",
    "df_nlp_final = df_nlp_final.reset_index(drop=True)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: EXPORT\n",
    "# ==========================================\n",
    "output_filename = 'nlp_database_CLEAN_V8.csv'\n",
    "df_nlp_final.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Process Complete. Cleaned database saved to: {output_filename}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: VERIFICATION (POST-CONDITION CHECK)\n",
    "# ==========================================\n",
    "print(\"\\nFinal Verification:\")\n",
    "check_list = [\"etek\", \"nee\", \"6060\", \"0002\", \"ees\", \"MERICAN\"]\n",
    "clean_count = 0\n",
    "\n",
    "for item in check_list:\n",
    "    # check if the artifact persists in the text column\n",
    "    matches = df_nlp_final[df_nlp_final['text'].str.contains(r'\\b' + item + r'\\b', case=False, na=False)]\n",
    "    if len(matches) > 0:\n",
    "        print(f\"FAILURE: Artifact '{item}' persists in the corpus.\")\n",
    "    else:\n",
    "        print(f\"SUCCESS: Artifact '{item}' has been eliminated.\")\n",
    "        clean_count += 1\n",
    "\n",
    "if clean_count == len(check_list):\n",
    "    print(\"\\nVERIFICATION SUCCESSFUL. The dataset is ready for Sentiment Analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
