{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841740ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.14.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/xavierfoidart/Library/Python/3.13/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/xavierfoidart/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c4cff",
   "metadata": {},
   "source": [
    "Context: This script initializes the Natural Language Processing (NLP) workflow within the 02b_cleaning_speeches.ipynb notebook. Its primary objective is to construct a text corpus comprising the nomination acceptance speeches of Democratic and Republican presidential candidates from 2000 to 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78fcf614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data for: Harris (2024)...\n",
      "Retrieving data for: Trump (2024)...\n",
      "Retrieving data for: Biden (2020)...\n",
      "Retrieving data for: Trump (2020)...\n",
      "Retrieving data for: Clinton (2016)...\n",
      "Retrieving data for: Trump (2016)...\n",
      "Retrieving data for: Obama (2012)...\n",
      "Retrieving data for: Romney (2012)...\n",
      "Retrieving data for: Obama (2008)...\n",
      "Retrieving data for: McCaine (2008)...\n",
      "Retrieving data for: Kerry (2004)...\n",
      "Retrieving data for: Bush (2004)...\n",
      "Retrieving data for: Gore (2000)...\n",
      "Retrieving data for: Bush (2000)...\n",
      "Scraping complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>party</th>\n",
       "      <th>candidate</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Harris</td>\n",
       "      <td>The Vice President: Good evening! [ Laughs. ] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Thank you very much. Thank you very, very much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Biden</td>\n",
       "      <td>Good evening. Ella Baker, a giant of the civil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Thank you very much. Thank you very much. Than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>Thank you all very, very much! Thank you for t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       party candidate  \\\n",
       "0  2024    Democrat    Harris   \n",
       "1  2024  Republican     Trump   \n",
       "2  2020    Democrat     Biden   \n",
       "3  2020  Republican     Trump   \n",
       "4  2016    Democrat   Clinton   \n",
       "\n",
       "                                                text  \n",
       "0  The Vice President: Good evening! [ Laughs. ] ...  \n",
       "1  Thank you very much. Thank you very, very much...  \n",
       "2  Good evening. Ella Baker, a giant of the civil...  \n",
       "3  Thank you very much. Thank you very much. Than...  \n",
       "4  Thank you all very, very much! Thank you for t...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: INITIALIZATION OF SCRAPING TARGETS\n",
    "# ==========================================\n",
    "# Define a list of dictionaries containing metadata and target URLs for presidential nomination acceptance speeches.\n",
    "# Source: The American Presidency Project (UCSB).\n",
    "urls_to_scrape = [\n",
    "    {\"year\": 2024, \"party\": \"Democrat\", \"candidate\": \"Harris\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-democratic-presidential-nomination-chicago-illinois\"},\n",
    "    {\"year\": 2024, \"party\": \"Republican\", \"candidate\": \"Trump\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-republican-national-convention-milwaukee\"},\n",
    "    {\"year\": 2020, \"party\": \"Democrat\", \"candidate\": \"Biden\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-democratic-presidential-nomination-wilmington-delaware\"},\n",
    "    {\"year\": 2020, \"party\": \"Republican\", \"candidate\": \"Trump\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-republican-presidential-nomination-2\"},\n",
    "    {\"year\": 2016, \"party\": \"Democrat\", \"candidate\": \"Clinton\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-democratic-national-convention\"},\n",
    "    {\"year\": 2016, \"party\": \"Republican\", \"candidate\": \"Trump\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-republican-national-convention-cleveland\"},\n",
    "    {\"year\": 2012, \"party\": \"Democrat\", \"candidate\": \"Obama\", \"url\": \"https://www.presidency.ucsb.edu/documents/remarks-accepting-the-presidential-nomination-the-democratic-national-convention-charlotte\"},\n",
    "    {\"year\": 2012, \"party\": \"Republican\", \"candidate\": \"Romney\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-republican-national-convention-tampa\"},\n",
    "    {\"year\": 2008, \"party\": \"Democrat\", \"candidate\": \"Obama\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-democratic-national-convention-denver\"},\n",
    "    {\"year\": 2008, \"party\": \"Republican\", \"candidate\": \"McCaine\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-republican-national-convention-saint\"},\n",
    "    {\"year\": 2004, \"party\": \"Democrat\", \"candidate\": \"Kerry\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-democratic-national-convention-boston\"},\n",
    "    {\"year\": 2004, \"party\": \"Republican\", \"candidate\": \"Bush\", \"url\": \"https://www.presidency.ucsb.edu/documents/remarks-accepting-the-presidential-nomination-the-republican-national-convention-new-york\"},\n",
    "    {\"year\": 2000, \"party\": \"Democrat\", \"candidate\": \"Gore\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-democratic-national-convention-los\"},\n",
    "    {\"year\": 2000, \"party\": \"Republican\", \"candidate\": \"Bush\", \"url\": \"https://www.presidency.ucsb.edu/documents/address-accepting-the-presidential-nomination-the-republican-national-convention-0\"},\n",
    "]\n",
    "\n",
    "speech_data = []\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: WEB SCRAPING LOOP\n",
    "# ==========================================\n",
    "# Iterate through each candidate's URL to retrieve the raw HTML content.\n",
    "for item in urls_to_scrape:\n",
    "    print(f\"Retrieving data for: {item['candidate']} ({item['year']})...\")\n",
    "    \n",
    "    try:\n",
    "        # Send an HTTP GET request to the target URL.\n",
    "        response = requests.get(item['url'])\n",
    "        \n",
    "        # Validate the HTTP status code (200 OK) before proceeding.\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content using BeautifulSoup.\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # DOM Traversal: Locate the specific 'div' container holding the speech transcript.\n",
    "            # The class 'field-docs-content' is specific to the structure of The American Presidency Project website.\n",
    "            content_div = soup.find('div', class_='field-docs-content')\n",
    "            \n",
    "            if content_div:\n",
    "                # Text Extraction: Retrieve text content while stripping HTML tags and excess whitespace.\n",
    "                text = content_div.get_text(separator=' ', strip=True)\n",
    "                \n",
    "                # Append the structured data to the list.\n",
    "                speech_data.append({\n",
    "                    \"year\": item['year'],\n",
    "                    \"party\": item['party'],\n",
    "                    \"candidate\": item['candidate'],\n",
    "                    \"text\": text\n",
    "                })\n",
    "            else:\n",
    "                print(f\"ERROR: No text content found for {item['candidate']}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Broken link or server error for {item['candidate']}\")\n",
    "            \n",
    "        # Rate Limiting: Pause execution for 1 second to respect the server's request policies.\n",
    "        time.sleep(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: DATAFRAME CREATION\n",
    "# ==========================================\n",
    "# Convert the list of dictionaries into a pandas DataFrame for analysis.\n",
    "df_speeches = pd.DataFrame(speech_data)\n",
    "\n",
    "# Validation: Display the first few rows to verify data integrity.\n",
    "print(\"Scraping complete.\")\n",
    "df_speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754857ad",
   "metadata": {},
   "source": [
    "Context: This segment of the 02b_cleaning_speeches.ipynb notebook implements the text preprocessing phase, which is fundamental to ensuring the validity of subsequent Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7551eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW   : The Vice President: Good evening! [ Laughs. ] [ Applause. ] Audience: Kamala! Kamala! Kamala! The Vice President: California. [ Laughs. ] [ Applause. \n",
      "CLEAN : Good evening! Audience: Kamala! Kamala! Kamala! The Vice President: California. Good evening, everyone. Good evening. Good evening. Oh, my goodness. G\n",
      "\n",
      "✅ File 'president_speeches_clean.csv' successfully saved!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def super_cleaning(text):\n",
    "    \"\"\"\n",
    "    Performs rigorous text preprocessing to isolate the spoken content \n",
    "    from transcript metadata and non-verbal annotations.\n",
    "    \"\"\"\n",
    "    # 1. Remove non-verbal annotations enclosed in square brackets \n",
    "    # (e.g., [Applause], [Laughter]) using regex.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # 2. Remove content enclosed in parentheses.\n",
    "    # This often includes editor notes or additional non-verbal cues.\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    \n",
    "    # 3. Remove speaker attribution prefixes if present at the start.\n",
    "    # Checks the first 50 characters for a colon (e.g., \"The Vice President: ...\").\n",
    "    if \":\" in text[:50]: \n",
    "        text = text.split(\":\", 1)[1]\n",
    "    \n",
    "    # 4. Whitespace Normalization:\n",
    "    # Replace newline characters with spaces to ensure continuity.\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Collapse multiple whitespace characters into a single space and trim leading/trailing spaces.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the raw text column to generate a standardized dataset.\n",
    "df_speeches['clean_text'] = df_speeches['text'].apply(super_cleaning)\n",
    "\n",
    "# Validation: Compare a sample of the raw vs. cleaned text (first 150 characters).\n",
    "print(\"RAW   :\", df_speeches['text'].iloc[0][:150])\n",
    "print(\"CLEAN :\", df_speeches['clean_text'].iloc[0][:150])\n",
    "\n",
    "# Export the processed corpus to a CSV file for NLP analysis.\n",
    "df_speeches.to_csv(\"president_speeches_clean.csv\", index=False)\n",
    "print(\"\\n✅ File 'president_speeches_clean.csv' successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d225c76",
   "metadata": {},
   "source": [
    "Context: This code block represents a refinement stage within 02b_cleaning_speeches.ipynb. Having identified that the initial cleaning pass left residual artifacts (such as \"Audience:\" or moderator interventions), this script applies a more rigorous cleaning protocol to the president_speeches_clean.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0323b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process initiated...\n",
      "File 'president_speeches_clean.csv' successfully updated.\n",
      "The dataset now contains only the cleaned text version.\n",
      "\n",
      "Preview (first 50 characters):\n",
      "Good evening! Kamala! Kamala! Kamala! California. Good evening, everyone. Good evening. Good evening\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: LOAD INTERMEDIATE DATA\n",
    "# ==========================================\n",
    "# Load the dataset generated in the previous step.\n",
    "df = pd.read_csv('president_speeches_clean.csv')\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: DEFINE ADVANCED CLEANING FUNCTION\n",
    "# ==========================================\n",
    "def ultimate_cleaning(text):\n",
    "    \"\"\"\n",
    "    Performs a secondary, granular cleaning pass to remove specific speaker \n",
    "    attributions and residual transcriptional artifacts.\n",
    "    \"\"\"\n",
    "    # Validation: Ensure the input is a string; otherwise return an empty string.\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # Remove content enclosed in brackets [] and parentheses ().\n",
    "    # This targets non-verbal annotations such as applause or laughter.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    \n",
    "    # Remove specific speaker prefixes (e.g., \"Audience:\", \"The Vice President:\").\n",
    "    # This regex removes phrases resembling \"Title:\" at the beginning of a segment\n",
    "    # to ensure the text strictly reflects the candidate's speech.\n",
    "    text = re.sub(r'\\b(Audience|The Vice President|The President|Hon\\.|Mr\\.|Ms\\.|Mrs\\.)\\s*:', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Standardize whitespace:\n",
    "    # Replace newlines with spaces and collapse multiple spaces into a single instance.\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: APPLY CLEANING\n",
    "# ==========================================\n",
    "print(\"Cleaning process initiated...\")\n",
    "df['clean_text'] = df['text'].apply(ultimate_cleaning)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: FEATURE SELECTION\n",
    "# ==========================================\n",
    "# Select only the relevant metadata and the cleaned text column.\n",
    "# The raw text is discarded to optimize the dataset for NLP analysis.\n",
    "df_final = df[['year', 'party', 'candidate', 'clean_text']].copy()\n",
    "\n",
    "# Rename 'clean_text' to 'text' to standardize the column schema.\n",
    "df_final.rename(columns={'clean_text': 'text'}, inplace=True)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: SAVE FINAL DATASET\n",
    "# ==========================================\n",
    "# Overwrite the existing CSV file with the fully processed version.\n",
    "df_final.to_csv(\"president_speeches_clean.csv\", index=False)\n",
    "\n",
    "print(\"File 'president_speeches_clean.csv' successfully updated.\")\n",
    "print(\"The dataset now contains only the cleaned text version.\")\n",
    "print(\"\\nPreview (first 50 characters):\")\n",
    "print(df_final['text'].iloc[0][:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
