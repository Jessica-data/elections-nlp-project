{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f93751d",
   "metadata": {},
   "source": [
    "Context: This script initializes the Natural Language Processing (NLP) environment within the 05_nlp.ipynb notebook. Before analyzing the rhetorical strategies of presidential candidates, the necessary software infrastructure must be established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228a4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/anaconda3/lib/python3.13/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/textblob/download_corpora.py\", line 53, in <module>\n",
      "    main()\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/textblob/download_corpora.py\", line 48, in main\n",
      "    download_all()\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/textblob/download_corpora.py\", line 41, in download_all\n",
      "    nltk.download(each)\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/nltk/downloader.py\", line 774, in download\n",
      "    for msg in self.incr_download(info_or_id, download_dir, force):\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/nltk/downloader.py\", line 642, in incr_download\n",
      "    yield from self._download_package(info, download_dir, force)\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/nltk/downloader.py\", line 706, in _download_package\n",
      "    infile = urlopen(info.url)\n",
      "  File \"/opt/anaconda3/lib/python3.13/urllib/request.py\", line 189, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.13/urllib/request.py\", line 489, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/opt/anaconda3/lib/python3.13/urllib/request.py\", line 506, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"/opt/anaconda3/lib/python3.13/urllib/request.py\", line 466, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.13/urllib/request.py\", line 1367, in https_open\n",
      "    return self.do_open(http.client.HTTPSConnection, req,\n",
      "  File \"/opt/anaconda3/lib/python3.13/urllib/request.py\", line 1323, in do_open\n",
      "    r = h.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.13/http/client.py\", line 1430, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.13/http/client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.13/http/client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.13/ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.13/ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 1: ENVIRONMENT SETUP & DEPENDENCY INSTALLATION\n",
    "# ==========================================\n",
    "# Note: The exclamation mark (!) allows the execution of shell commands directly within the Jupyter Notebook.\n",
    "\n",
    "# 1. Install TextBlob\n",
    "# We utilize 'TextBlob', a Python library built upon NLTK (Natural Language Toolkit), \n",
    "# chosen for its efficiency in performing standard NLP tasks such as sentiment analysis \n",
    "# and noun phrase extraction.\n",
    "!pip install textblob\n",
    "\n",
    "# 2. Download Linguistic Corpora\n",
    "# TextBlob requires specific lexical resources (corpora) to function correctly.\n",
    "# This command downloads the necessary datasets, including 'punkt' (for tokenization) \n",
    "# and 'averaged_perceptron_tagger' (for part-of-speech tagging).\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a1cbf",
   "metadata": {},
   "source": [
    "Context: This script constitutes the core analytical engine of the 05_nlp.ipynb notebook. It transitions from raw text processing to quantitative Feature Extraction. The goal is to convert unstructured text data (speeches and manifestos) into numerical vectors (Sentiment and Subjectivity scores) that can be analyzed statistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd4f2068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ V8 Cleaned Database successfully loaded.\n",
      "Document Type Distribution:\n",
      "source_type\n",
      "Speech      14\n",
      "Platform    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üß† Initiating Sentiment Analysis...\n",
      "\n",
      "--- DETAILED FINAL RESULTS ---\n",
      "    year       party  sentiment_speech  sentiment_platform  sentiment_mean\n",
      "12  2024    Democrat          0.188678            0.107454        0.148066\n",
      "13  2024  Republican          0.176667            0.089143        0.132905\n",
      "10  2020    Democrat          0.154585            0.103591        0.129088\n",
      "11  2020  Republican          0.157135                 NaN        0.157135\n",
      "8   2016    Democrat          0.172119            0.131665        0.151892\n",
      "9   2016  Republican          0.096610            0.097079        0.096845\n",
      "6   2012    Democrat          0.157531            0.143446        0.150488\n",
      "7   2012  Republican          0.193518            0.106685        0.150101\n",
      "4   2008    Democrat          0.152744            0.123579        0.138161\n",
      "5   2008  Republican          0.151983            0.122194        0.137089\n",
      "\n",
      "‚úÖ File 'political_sentiment_DETAILED.csv' generated successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: DATA LOADING\n",
    "# ==========================================\n",
    "# Load the sanitized NLP dataset (V8 Cleaned Version).\n",
    "# This file contains the unified corpus of speeches and manifestos.\n",
    "try:\n",
    "    df = pd.read_csv('/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/processed/nlp_database_CLEAN_V8.csv')\n",
    "    print(\"‚úÖ V8 Cleaned Database successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback mechanism if the specific path fails\n",
    "    df = pd.read_csv('nlp_database_CLEAN_TERMINATOR.csv')\n",
    "    print(\"‚úÖ Alternate Database loaded.\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: DOCUMENT TYPOLOGY IDENTIFICATION\n",
    "# ==========================================\n",
    "# Distinguish between \"Speeches\" (Candidate Rhetoric) and \"Platforms\" (Party Ideology).\n",
    "# We generate a categorical variable 'source_type' to facilitate comparative analysis.\n",
    "def get_type(candidate_name):\n",
    "    if \"Party Platform\" in str(candidate_name):\n",
    "        return \"Platform\"\n",
    "    else:\n",
    "        return \"Speech\"\n",
    "\n",
    "df['source_type'] = df['candidate'].apply(get_type)\n",
    "\n",
    "print(\"Document Type Distribution:\")\n",
    "print(df['source_type'].value_counts())\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: SENTIMENT & SUBJECTIVITY SCORING (TEXTBLOB)\n",
    "# ==========================================\n",
    "print(\"\\nüß† Initiating Sentiment Analysis...\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Returns the Polarity score: Float within range [-1.0, 1.0]\n",
    "    # -1.0 = Highly Negative | 0 = Neutral | +1.0 = Highly Positive\n",
    "    return TextBlob(str(text)).sentiment.polarity\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    # Returns the Subjectivity score: Float within range [0.0, 1.0]\n",
    "    # 0.0 = Objective/Factual | 1.0 = Subjective/Opinionated\n",
    "    return TextBlob(str(text)).sentiment.subjectivity\n",
    "\n",
    "# Apply the functions to the text corpus\n",
    "df['sentiment'] = df['text'].apply(get_sentiment)\n",
    "df['subjectivity'] = df['text'].apply(get_subjectivity)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: DATA RESTRUCTURING (PIVOTING)\n",
    "# ==========================================\n",
    "# Transform the dataset from Long Format to Wide Format.\n",
    "# Objective: Create distinct columns for 'Speech' and 'Platform' metrics for each Year-Party pair.\n",
    "df_pivot = df.pivot_table(\n",
    "    index=['year', 'party'], \n",
    "    columns='source_type', \n",
    "    values=['sentiment', 'subjectivity'],\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the hierarchical MultiIndex columns created by the pivot table.\n",
    "# Renaming schema: e.g., ('sentiment', 'Speech') becomes 'sentiment_speech'\n",
    "df_pivot.columns = ['year', 'party', \n",
    "                    'sentiment_platform', 'sentiment_speech', \n",
    "                    'subjectivity_platform', 'subjectivity_speech']\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: AGGREGATE METRIC CALCULATION\n",
    "# ==========================================\n",
    "# Compute a composite score (Mean) combining both oral rhetoric (Speech) and written policy (Platform).\n",
    "# This provides a holistic view of the party's tone for that election cycle.\n",
    "df_pivot['sentiment_mean'] = df_pivot[['sentiment_platform', 'sentiment_speech']].mean(axis=1)\n",
    "df_pivot['subjectivity_mean'] = df_pivot[['subjectivity_platform', 'subjectivity_speech']].mean(axis=1)\n",
    "\n",
    "# Chronological Sorting\n",
    "df_pivot = df_pivot.sort_values(by=['year', 'party'], ascending=[False, True])\n",
    "\n",
    "# ==========================================\n",
    "# STEP 6: VALIDATION & EXPORT\n",
    "# ==========================================\n",
    "print(\"\\n--- DETAILED FINAL RESULTS ---\")\n",
    "# Display a comparative snapshot of the sentiment metrics\n",
    "print(df_pivot[['year', 'party', 'sentiment_speech', 'sentiment_platform', 'sentiment_mean']].head(10))\n",
    "\n",
    "# Export the processed sentiment dataset for integration with the predictive model.\n",
    "output_name = 'political_sentiment_DETAILED.csv'\n",
    "df_pivot.to_csv(output_name, index=False)\n",
    "print(f\"\\n‚úÖ File '{output_name}' generated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
