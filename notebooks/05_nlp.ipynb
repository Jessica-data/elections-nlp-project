{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f93751d",
   "metadata": {},
   "source": [
    "Context: This script initializes the Natural Language Processing (NLP) environment within the 05_nlp.ipynb notebook. Before analyzing the rhetorical strategies of presidential candidates, the necessary software infrastructure must be established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9228a4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.13/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/anaconda3/lib/python3.13/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/jessicabourdouxhe/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 1: ENVIRONMENT SETUP & DEPENDENCY INSTALLATION\n",
    "# ==========================================\n",
    "# Note: The exclamation mark (!) allows the execution of shell commands directly within the Jupyter Notebook.\n",
    "\n",
    "# 1. Install TextBlob\n",
    "# We utilize 'TextBlob', a Python library built upon NLTK (Natural Language Toolkit), \n",
    "# chosen for its efficiency in performing standard NLP tasks such as sentiment analysis \n",
    "# and noun phrase extraction.\n",
    "!pip install textblob\n",
    "\n",
    "# 2. Download Linguistic Corpora\n",
    "# TextBlob requires specific lexical resources (corpora) to function correctly.\n",
    "# This command downloads the necessary datasets, including 'punkt' (for tokenization) \n",
    "# and 'averaged_perceptron_tagger' (for part-of-speech tagging).\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a1cbf",
   "metadata": {},
   "source": [
    "Context: This script constitutes the core analytical engine of the 05_nlp.ipynb notebook. It transitions from raw text processing to quantitative Feature Extraction. The goal is to convert unstructured text data (speeches and manifestos) into numerical vectors (Sentiment and Subjectivity scores) that can be analyzed statistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4f2068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ V8 Cleaned Database successfully loaded.\n",
      "Document Type Distribution:\n",
      "source_type\n",
      "Speech      14\n",
      "Platform    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üß† Initiating Sentiment Analysis...\n",
      "\n",
      "--- DETAILED FINAL RESULTS ---\n",
      "    year       party  sentiment_speech  sentiment_platform  sentiment_mean\n",
      "12  2024    Democrat          0.188678            0.107454        0.148066\n",
      "13  2024  Republican          0.176667            0.089143        0.132905\n",
      "10  2020    Democrat          0.154585            0.103591        0.129088\n",
      "11  2020  Republican          0.157135                 NaN        0.157135\n",
      "8   2016    Democrat          0.172119            0.131665        0.151892\n",
      "9   2016  Republican          0.096610            0.097079        0.096845\n",
      "6   2012    Democrat          0.157531            0.143446        0.150488\n",
      "7   2012  Republican          0.193518            0.106685        0.150101\n",
      "4   2008    Democrat          0.152744            0.123579        0.138161\n",
      "5   2008  Republican          0.151983            0.122194        0.137089\n",
      "\n",
      "‚úÖ File 'political_sentiment_DETAILED.csv' generated successfully!\n",
      "üìç Location: /Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/processed/political_sentiment_DETAILED.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: DATA LOADING\n",
    "# ==========================================\n",
    "# Load the sanitized NLP dataset (V8 Cleaned Version).\n",
    "# This file contains the unified corpus of speeches and manifestos.\n",
    "current_dir = Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "FIG_DIR = PROJECT_ROOT / 'figures'\n",
    "\n",
    "file_path = PROCESSED_DIR / \"nlp_database_CLEAN_V8.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"‚úÖ V8 Cleaned Database successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback mechanism if the specific path fails\n",
    "    df = pd.read_csv('nlp_database_CLEAN_TERMINATOR.csv')\n",
    "    print(\"‚úÖ Alternate Database loaded.\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: DOCUMENT TYPOLOGY IDENTIFICATION\n",
    "# ==========================================\n",
    "# Distinguish between \"Speeches\" (Candidate Rhetoric) and \"Platforms\" (Party Ideology).\n",
    "# We generate a categorical variable 'source_type' to facilitate comparative analysis.\n",
    "def get_type(candidate_name):\n",
    "    if \"Party Platform\" in str(candidate_name):\n",
    "        return \"Platform\"\n",
    "    else:\n",
    "        return \"Speech\"\n",
    "\n",
    "df['source_type'] = df['candidate'].apply(get_type)\n",
    "\n",
    "print(\"Document Type Distribution:\")\n",
    "print(df['source_type'].value_counts())\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: SENTIMENT & SUBJECTIVITY SCORING (TEXTBLOB)\n",
    "# ==========================================\n",
    "print(\"\\nüß† Initiating Sentiment Analysis...\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Returns the Polarity score: Float within range [-1.0, 1.0]\n",
    "    # -1.0 = Highly Negative | 0 = Neutral | +1.0 = Highly Positive\n",
    "    return TextBlob(str(text)).sentiment.polarity\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    # Returns the Subjectivity score: Float within range [0.0, 1.0]\n",
    "    # 0.0 = Objective/Factual | 1.0 = Subjective/Opinionated\n",
    "    return TextBlob(str(text)).sentiment.subjectivity\n",
    "\n",
    "# Apply the functions to the text corpus\n",
    "df['sentiment'] = df['text'].apply(get_sentiment)\n",
    "df['subjectivity'] = df['text'].apply(get_subjectivity)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: DATA RESTRUCTURING (PIVOTING)\n",
    "# ==========================================\n",
    "# Transform the dataset from Long Format to Wide Format.\n",
    "# Objective: Create distinct columns for 'Speech' and 'Platform' metrics for each Year-Party pair.\n",
    "df_pivot = df.pivot_table(\n",
    "    index=['year', 'party'], \n",
    "    columns='source_type', \n",
    "    values=['sentiment', 'subjectivity'],\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the hierarchical MultiIndex columns created by the pivot table.\n",
    "# Renaming schema: e.g., ('sentiment', 'Speech') becomes 'sentiment_speech'\n",
    "df_pivot.columns = ['year', 'party', \n",
    "                    'sentiment_platform', 'sentiment_speech', \n",
    "                    'subjectivity_platform', 'subjectivity_speech']\n",
    "\n",
    "# ==========================================\n",
    "# STEP 5: AGGREGATE METRIC CALCULATION\n",
    "# ==========================================\n",
    "# Compute a composite score (Mean) combining both oral rhetoric (Speech) and written policy (Platform).\n",
    "# This provides a holistic view of the party's tone for that election cycle.\n",
    "df_pivot['sentiment_mean'] = df_pivot[['sentiment_platform', 'sentiment_speech']].mean(axis=1)\n",
    "df_pivot['subjectivity_mean'] = df_pivot[['subjectivity_platform', 'subjectivity_speech']].mean(axis=1)\n",
    "\n",
    "# Chronological Sorting\n",
    "df_pivot = df_pivot.sort_values(by=['year', 'party'], ascending=[False, True])\n",
    "\n",
    "# ==========================================\n",
    "# STEP 6: VALIDATION & EXPORT\n",
    "# ==========================================\n",
    "print(\"\\n--- DETAILED FINAL RESULTS ---\")\n",
    "# Display a comparative snapshot of the sentiment metrics\n",
    "print(df_pivot[['year', 'party', 'sentiment_speech', 'sentiment_platform', 'sentiment_mean']].head(10))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: EXPORT PROCESSED DATA (Sentiment Analysis)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Configuration des chemins\n",
    "current_dir = Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# 2. S√©curit√© : Cr√©ation du dossier s'il n'existe pas\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. D√©finition du chemin complet\n",
    "output_name = 'political_sentiment_DETAILED.csv'\n",
    "save_path_sentiment = PROCESSED_DIR / output_name\n",
    "\n",
    "# 4. Export\n",
    "df_pivot.to_csv(save_path_sentiment, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ File '{output_name}' generated successfully!\")\n",
    "print(f\"üìç Location: {save_path_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
