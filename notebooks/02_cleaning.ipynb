{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76aa8752",
   "metadata": {},
   "source": [
    "Context: This script constitutes the initial data preprocessing phase within the 02_cleaning.ipynb notebook. The primary objective is to transform raw election data into a structured format suitable for the subsequent predictive modeling of U.S. elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a803c3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/election_results/countypres_2000-2024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# STEP 1: DATA LOADING\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Import the raw dataset containing county-level presidential election results (2000-2024).\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# The Python engine is specified to ensure robust handling of the separator detection.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/election_results/countypres_2000-2024.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ File successfully loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_raw\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/election_results/countypres_2000-2024.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: DATA LOADING\n",
    "# ==========================================\n",
    "# Import the raw dataset containing county-level presidential election results (2000-2024).\n",
    "# The Python engine is specified to ensure robust handling of the separator detection.\n",
    "file_path = '/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/election_results/countypres_2000-2024.csv'\n",
    "df_raw = pd.read_csv(file_path, sep=None, engine='python')\n",
    "\n",
    "print(\"‚úÖ File successfully loaded.\")\n",
    "print(f\"Detected columns: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# Display the initial dimensions of the dataset to verify import integrity.\n",
    "print(f\"Initial dataset shape: {df_raw.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: DATA CLEANING & FILTERING\n",
    "# ==========================================\n",
    "# Filter the dataset to retain only the two major political parties (Democrat and Republican).\n",
    "# This exclusion of minor parties (e.g., Green, Libertarian) focuses the analysis on the primary electoral competition.\n",
    "df_clean = df_raw[df_raw['party'].isin(['DEMOCRAT', 'REPUBLICAN'])].copy()\n",
    "print(\"‚úÖ Filtering complete.\")\n",
    "\n",
    "# Feature Engineering: Calculate 'vote_share' to normalize vote counts.\n",
    "# This metric represents the proportion of total votes a candidate received within a specific county.\n",
    "# Normalization allows for comparable analysis across counties with significant population disparities.\n",
    "df_clean['vote_share'] = df_clean['candidatevotes'] / df_clean['totalvotes']\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: DATA RESHAPING (PIVOTING)\n",
    "# ==========================================\n",
    "# Transform the dataset structure from \"Long Format\" (observation per candidate) \n",
    "# to \"Wide Format\" (observation per county per election year).\n",
    "# This restructuring aligns the data for machine learning, creating distinct features for Democratic and Republican performance.\n",
    "df_pivot = df_clean.pivot_table(\n",
    "    index=['year', 'state_po', 'county_name', 'county_fips'], # Unique identifiers for the observation unit\n",
    "    columns='party',                                          # Categorical values transformed into column headers\n",
    "    values='vote_share'                                       # Metric to populate the new fields\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the hierarchical column structure resulting from the pivot operation for cleaner access.\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "# Impute missing values with 0 to account for counties where a specific party received no recorded votes.\n",
    "df_pivot = df_pivot.fillna(0)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: EXPORT & VALIDATION (Votes)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Configuration des chemins\n",
    "current_dir = Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# 2. S√©curit√© : Cr√©ation du dossier s'il n'existe pas\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. D√©finition du chemin complet\n",
    "output_filename = \"votes_cleaned_2000_2024.csv\"\n",
    "save_path_votes = PROCESSED_DIR / output_filename\n",
    "\n",
    "# 4. Export\n",
    "# Export the processed dataset to a CSV file for use in subsequent modeling phases.\n",
    "df_pivot.to_csv(save_path_votes, index=False)\n",
    "\n",
    "print(f\"Processing complete. Data saved to '{save_path_votes}'\")\n",
    "print(\"\\nFirst 5 rows of the cleaned dataset:\")\n",
    "print(df_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be9378",
   "metadata": {},
   "source": [
    "We clean and filter the dataset in order to keep only the two major political parties (Democrats and Republicans). We calculate vote share to normalize the vote count. Then we transforme the dataset to from \"Observation per candidate\" to a \"county-year observation\". We impute missing values with 0 to account for counties where a specific party received no recorded votes. Finally, we export the clean dataset \"votes_cleaned_2000_2024.csv\" to /data/processed folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a569e",
   "metadata": {},
   "source": [
    "Context: This script processes the DP03_economic.csv file within the 02_cleaning.ipynb notebook. It focuses on preparing socio-economic predictors that will serve as independent variables in the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f337363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/2bt1kt6d0xzb95k0cn4jftsr0000gn/T/ipykernel_34064/3194104859.py:11: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,144,145,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,276,278,279,280,281,282,283,284,285,286,287,288,289,290,292,293,294,296,297,298,299,300,301,302,304,305,306,308,309,310,312,313,314,315,316,317,318,319,320,321,322,323,326,328,329,330,331,332,333,334,335,336,337,338,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,368,369,370,371,372,373,374,375,376,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,402,403,406,407,410,411,414,415,418,419,422,423,424,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,452,464,466,467,468,469,470,471,472,473,474,476,477,478,480,482,484,485,486,487,488,489,490,491,492,502,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,524,525,526,527,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_eco = pd.read_csv(file_path_eco, header=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ File 'political_sentiment_DETAILED.csv' generated successfully!\n",
      "üìç Location: /Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/processed/political_sentiment_DETAILED.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: LOAD DATA\n",
    "# ==========================================\n",
    "# Define the file path for the raw socio-economic dataset (Economic Characteristics).\n",
    "file_path_eco = \"/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/socio-economic/DP03_economic.csv\"\n",
    "\n",
    "# Load the dataset. 'header=0' is specified to correctly identify the initial header row.\n",
    "df_eco = pd.read_csv(file_path_eco, header=0)\n",
    "\n",
    "# Remove the second row (index 0 after loading), which typically contains metadata descriptions \n",
    "# rather than actual observations in US Census datasets.\n",
    "df_eco = df_eco.iloc[1:] \n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: VARIABLE SELECTION (DIMENSIONALITY REDUCTION)\n",
    "# ==========================================\n",
    "# Select a subset of relevant independent variables to mitigate the risk of overfitting.\n",
    "# The selection focuses on key economic indicators: Unemployment, Income, Poverty, and Public Sector employment.\n",
    "# 'DP03_0043PE' refers to the percentage of government workers.\n",
    "\n",
    "columns_mapping = {\n",
    "    'GEO_ID': 'fips',\n",
    "    'NAME': 'county_name',\n",
    "    'DP03_0009PE': 'unemployment_rate',      # Unemployment rate\n",
    "    'DP03_0062E': 'median_income',           # Median household income\n",
    "    'DP03_0128PE': 'poverty_rate',           # Poverty rate\n",
    "    'DP03_0043PE': 'public_workers_pct'      # (BONUS) Percentage of public sector workers\n",
    "}\n",
    "\n",
    "# Create a new dataframe containing only the selected variables and rename them for clarity.\n",
    "df_eco_clean = df_eco[columns_mapping.keys()].rename(columns=columns_mapping).copy()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: DATA CLEANING & TYPE CONVERSION\n",
    "# ==========================================\n",
    "# Convert economic indicators from object/string types to numeric formats to facilitate statistical analysis.\n",
    "# 'errors=coerce' converts non-numeric values to NaN (Not a Number).\n",
    "cols_to_convert = ['unemployment_rate', 'median_income', 'poverty_rate', 'public_workers_pct']\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    df_eco_clean[col] = pd.to_numeric(df_eco_clean[col], errors='coerce')\n",
    "\n",
    "# Standardize the Federal Information Processing Standard (FIPS) codes.\n",
    "# This extracts the last 5 digits to ensure consistency with other datasets (removing the '0500000US' prefix).\n",
    "df_eco_clean['fips'] = df_eco_clean['fips'].astype(str).str[-5:]\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: EXPORT PROCESSED DATA (Sentiment Analysis)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Configuration des chemins\n",
    "current_dir = Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# 2. S√©curit√© : Cr√©ation du dossier s'il n'existe pas\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. D√©finition du chemin complet\n",
    "output_name = 'political_sentiment_DETAILED.csv'\n",
    "save_path_sentiment = PROCESSED_DIR / output_name\n",
    "\n",
    "# 4. Export\n",
    "df_pivot.to_csv(save_path_sentiment, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ File '{output_name}' generated successfully!\")\n",
    "print(f\"üìç Location: {save_path_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f49c4",
   "metadata": {},
   "source": [
    "Our DP03_economic represents our economic variables over the studied period. Since this dataset contains many variables, we firstly decide to keep a subset of relevant variables (FIPS number of the county, county name, unemployment rate, median income, poverty rate, public workers). We therefore create a new dataframe containing only the selected variables. Our next step was to convert economic factors from object/strings into numeric format to facilitate the later analysis. We then needed to extract the five last digits of the FIPS code to ensure that the matching can be done with our other datsets which have a \"5 digits\" FIPS code. We finish by exporting the clean dataset into our /data/processed folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37bed3",
   "metadata": {},
   "source": [
    "Context: This script processes the DP02_socio.csv file within the 02_cleaning.ipynb notebook. It isolates specific social demographic indicators‚Äîspecifically educational attainment‚Äîfor use in the predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb9644c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/2bt1kt6d0xzb95k0cn4jftsr0000gn/T/ipykernel_34064/3839323164.py:11: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,344,346,347,348,349,350,351,352,353,354,355,356,357,358,360,361,362,363,364,365,366,367,368,369,370,372,373,374,375,376,377,378,379,380,381,382,384,385,396,398,399,400,401,402,403,404,405,406,407,408,414,416,417,418,419,420,421,422,423,424,425,426,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,448,449,450,452,453,454,456,457,458,460,461,462,464,465,466,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,486,487,488,489,490,491,492,493,494,495,496,497,498,504,506,508,509,510,511,512,518,532,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,614,615,616,617) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_socio = pd.read_csv(file_path_socio, header=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social Data Loaded. Dimensions: (3222, 619)\n",
      "Process Complete. File saved: /Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/processed/education_cleaned_2023.csv\n",
      "    fips  education_bachelors_pct\n",
      "1  01001                     28.3\n",
      "2  01003                     32.8\n",
      "3  01005                     11.5\n",
      "4  01007                     11.5\n",
      "5  01009                     15.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: LOAD SOCIAL DATA (DP02)\n",
    "# ==========================================\n",
    "# Define the file path for the raw social characteristics dataset (DP02).\n",
    "# Note: Ensure the path is correctly directed to the local machine environment.\n",
    "file_path_socio = \"/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/socio-economic/DP02_socio.csv\"\n",
    "\n",
    "# Load the dataset with the header located at the first row (index 0).\n",
    "df_socio = pd.read_csv(file_path_socio, header=0)\n",
    "\n",
    "# Exclude the second row (index 0 after load), which contains descriptive metadata \n",
    "# rather than statistical observations.\n",
    "df_socio = df_socio.iloc[1:] \n",
    "\n",
    "print(f\"Social Data Loaded. Dimensions: {df_socio.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: VARIABLE SELECTION (EDUCATION)\n",
    "# ==========================================\n",
    "# Feature Selection: Focus on educational attainment as a predictor.\n",
    "# Variable 'DP02_0068PE' represents the percentage of the population aged 25 years \n",
    "# and over who hold a Bachelor's degree or higher.\n",
    "\n",
    "columns_mapping_socio = {\n",
    "    'GEO_ID': 'fips',\n",
    "    'DP02_0068PE': 'education_bachelors_pct'\n",
    "}\n",
    "\n",
    "# Create a subset dataframe with the selected education variable and renamed columns.\n",
    "df_socio_clean = df_socio[columns_mapping_socio.keys()].rename(columns=columns_mapping_socio).copy()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: DATA CLEANING & STANDARDIZATION\n",
    "# ==========================================\n",
    "# Data Type Conversion: Transform the education percentage column from string to numeric \n",
    "# format to enable quantitative analysis. Invalid parsing will result in NaN.\n",
    "df_socio_clean['education_bachelors_pct'] = pd.to_numeric(df_socio_clean['education_bachelors_pct'], errors='coerce')\n",
    "\n",
    "# Primary Key Standardization: Extract the last 5 characters of the 'fips' code \n",
    "# to remove the '0500000US' prefix and ensure consistency with the election dataset.\n",
    "df_socio_clean['fips'] = df_socio_clean['fips'].astype(str).str[-5:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: EXPORT PROCESSED DATA\n",
    "# ==========================================\n",
    "current_dir = Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "FIG_DIR = PROJECT_ROOT / 'figures'\n",
    "\n",
    "\n",
    "# 2. \"Petite s√©curit√©\" : Cr√©er le dossier s'il n'existe pas\n",
    "# parents=True permet de cr√©er \"data\" si seul \"processed\" manquait\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. D√©finir le chemin complet du fichier\n",
    "output_filename_socio = \"education_cleaned_2023.csv\"\n",
    "save_path = PROCESSED_DIR / output_filename_socio\n",
    "\n",
    "# 4. Sauvegarder le fichier CSV\n",
    "df_socio_clean.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Process Complete. File saved: {save_path}\")\n",
    "print(df_socio_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef349f9c",
   "metadata": {},
   "source": [
    "Our DP02_socio represents our social variables over the studied period. Since this dataset contains many variables, we firstly decide to strictly focus on education (represents the percentage of the population aged 25 years and over who hold a Bachelor's degree or higher) . Our next step was to convert that variable from object/strings into numeric format to facilitate the later analysis. We then needed to extract the five last digits of the FIPS code to ensure that the matching can be done with our other datsets which have a \"5 digits\" FIPS code. We finish by exporting the clean dataset into our /data/processed folder. \n",
    "As an example, we can see that in county 01001, 28.3% of the population aged 25 years and over hold a Bachelor's degree or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e0e08",
   "metadata": {},
   "source": [
    "Context: This script processes the DP05_demo.csv file within the 02_cleaning.ipynb notebook. It is dedicated to preparing demographic features, specifically age and ethnicity, which are critical components of the socio-economic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc45141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Data Loaded. Dimensions: (3222, 379)\n",
      "Processing Complete. File saved: /Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/processed/demographics_cleaned_2023.csv\n",
      "    fips  median_age  white_pct  black_pct  hispanic_pct\n",
      "1  01001        39.2       73.6       20.0           0.8\n",
      "2  01003        43.7       82.8        8.0           1.6\n",
      "3  01005        40.7       44.0       46.9           0.9\n",
      "4  01007        41.3       75.1       20.7           1.9\n",
      "5  01009        40.9       89.5        1.3           1.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/2bt1kt6d0xzb95k0cn4jftsr0000gn/T/ipykernel_34064/1341232720.py:11: DtypeWarning: Columns (2,4,6,8,10,12,13,14,15,16,18,20,22,24,26,27,28,29,30,32,33,34,35,36,37,38,40,41,42,44,45,46,47,48,50,52,54,56,58,60,62,66,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,138,139,140,141,142,143,144,145,146,147,148,149,150,152,154,155,156,157,158,159,160,161,162,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,192,194,198,200,201,202,203,204,206,208,210,212,214,215,216,217,218,220,221,222,223,226,228,229,230,232,233,234,235,236,238,240,242,246,248,250,254,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,326,327,328,329,330,331,332,333,334,335,336,337,338,340,342,343,344,345,346,347,348,349,350,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,372,374,375,376,377) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_demo = pd.read_csv(file_path_demo, header=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: LOAD DEMOGRAPHIC DATA (DP05)\n",
    "# ==========================================\n",
    "# Define the file path for the raw demographic dataset (DP05).\n",
    "# Note: This path references the local environment structure.\n",
    "file_path_demo = \"/Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/raw/socio-economic/DP05_demo.csv\"\n",
    "\n",
    "# Load the dataset using the first row as the header.\n",
    "df_demo = pd.read_csv(file_path_demo, header=0)\n",
    "\n",
    "# Remove the description row (index 0) to retain only statistical observations.\n",
    "df_demo = df_demo.iloc[1:]\n",
    "\n",
    "print(f\"Demographic Data Loaded. Dimensions: {df_demo.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: VARIABLE SELECTION\n",
    "# ==========================================\n",
    "# Selection of key demographic variables relevant to US political behavior analysis:\n",
    "# - DP05_0018E: Median Age (Age correlates with political spectrum positioning).\n",
    "# - DP05_0037PE: Percentage White population (White alone).\n",
    "# - DP05_0038PE: Percentage Black population (Black or African American alone).\n",
    "# - DP05_0071PE: Percentage Hispanic population (Hispanic or Latino).\n",
    "\n",
    "columns_mapping_demo = {\n",
    "    'GEO_ID': 'fips',\n",
    "    'DP05_0018E': 'median_age',\n",
    "    'DP05_0037PE': 'white_pct',\n",
    "    'DP05_0038PE': 'black_pct',\n",
    "    'DP05_0071PE': 'hispanic_pct'\n",
    "}\n",
    "\n",
    "# Subset the dataframe to selected variables and rename columns for consistency.\n",
    "df_demo_clean = df_demo[columns_mapping_demo.keys()].rename(columns=columns_mapping_demo).copy()\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: DATA CLEANING & STANDARDIZATION\n",
    "# ==========================================\n",
    "# Convert demographic indicators to numeric types to facilitate quantitative analysis.\n",
    "# Errors are coerced to NaN to handle non-numeric artifacts.\n",
    "cols_to_convert = ['median_age', 'white_pct', 'black_pct', 'hispanic_pct']\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    df_demo_clean[col] = pd.to_numeric(df_demo_clean[col], errors='coerce')\n",
    "\n",
    "# Standardize FIPS codes by extracting the last 5 digits to match the election dataset format.\n",
    "df_demo_clean['fips'] = df_demo_clean['fips'].astype(str).str[-5:]\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: EXPORT PROCESSED DATA (Demographics)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Configuration des chemins (Votre code standardis√©)\n",
    "current_dir = Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent  # Remonte d'un niveau (suppose que le script est dans src/ ou notebooks/)\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# 2. \"Petite s√©curit√©\" : Cr√©er le dossier s'il n'existe pas\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. D√©finir le chemin complet du fichier\n",
    "output_filename_demo = \"demographics_cleaned_2023.csv\"\n",
    "save_path_demo = PROCESSED_DIR / output_filename_demo\n",
    "\n",
    "# 4. Sauvegarder le fichier CSV\n",
    "df_demo_clean.to_csv(save_path_demo, index=False)\n",
    "\n",
    "print(f\"Processing Complete. File saved: {save_path_demo}\")\n",
    "print(df_demo_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979e5ba",
   "metadata": {},
   "source": [
    "Our DP05_demo represents our demographic variables over the studied period. Since this dataset contains many variables, we firstly decide to select relevent variables: Median Age spectrum positioning, Percentage White population, Percentage Black population (Black or African American alone), Percentage Hispanic population (Hispanic or Latino). Our next step was to convert those variable from object/strings into numeric format to facilitate the later analysis. We then needed to extract the five last digits of the FIPS code to ensure that the matching can be done with our other datsets which have a \"5 digits\" FIPS code. We finish by exporting the clean dataset into our /data/processed folder. \n",
    "As an example, we can see that in county 01001, 39.2% of the population has an age that correlates with the political spectrum positioning, 73,6% of the population is white, 20% is black and 0,8% is hispanic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e701998",
   "metadata": {},
   "source": [
    "Context: This script represents the culmination of the data preprocessing pipeline in 02_cleaning.ipynb. Its purpose is to aggregate the disparate cleaned datasets‚Äîelection results, economic indicators, educational attainment, and demographic profiles‚Äîinto a single, cohesive Master Table suitable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd674096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Standardizing FIPS codes... ---\n",
      "‚úÖ Validation: Votes FIPS Example (Expected: 01001) : '02001'\n",
      "‚úÖ Validation: Eco FIPS Example   (Expected: 01001) : '01001'\n",
      "\n",
      "Merging datasets...\n",
      "\n",
      "üìä Final Master Table Dimensions: 21742 rows.\n",
      "üéâ SAVED! The master file has been populated here: /Users/jessicabourdouxhe/Desktop/Master 1/Data/Projet /elections-nlp-project/data/processed/master_table_elections.csv\n",
      "   year state_po  county_name   fips  DEMOCRAT  REPUBLICAN  unemployment_rate  \\\n",
      "0  2000       AK  DISTRICT 13  02013  0.335012    0.485081                3.8   \n",
      "1  2000       AK  DISTRICT 16  02016  0.420277    0.422625                4.1   \n",
      "2  2000       AK  DISTRICT 20  02020  0.324429    0.523912                4.6   \n",
      "3  2000       AL      AUTAUGA  01001  0.287192    0.696943                2.5   \n",
      "4  2000       AL      BALDWIN  01003  0.247822    0.723654                3.2   \n",
      "\n",
      "   median_income  poverty_rate  public_workers_pct  education_bachelors_pct  \\\n",
      "0        72692.0          12.4                 1.9                     18.1   \n",
      "1       107344.0           9.2                 4.4                     17.5   \n",
      "2        98152.0           9.3                 9.5                     37.7   \n",
      "3        69841.0          10.7                10.1                     28.3   \n",
      "4        75019.0          10.5                10.1                     32.8   \n",
      "\n",
      "   median_age  white_pct  black_pct  hispanic_pct  \n",
      "0        41.8       16.6        6.0          43.8  \n",
      "1        39.1       24.6        5.3          11.4  \n",
      "2        34.9       58.3        5.3          13.4  \n",
      "3        39.2       73.6       20.0           0.8  \n",
      "4        43.7       82.8        8.0           1.6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: LOAD PROCESSED DATA\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# Load all datasets into DataFrames\n",
    "df_votes = pd.read_csv(path_votes)\n",
    "df_eco = pd.read_csv(path_eco)\n",
    "df_edu = pd.read_csv(path_edu)\n",
    "df_demo = pd.read_csv(path_demo)\n",
    "\n",
    "# Standardization: Rename the primary key in the votes dataframe to match the socio-economic files.\n",
    "df_votes = df_votes.rename(columns={'county_fips': 'fips'})\n",
    "\n",
    "# ==========================================\n",
    "# üö® STEP 1.5: FIPS STANDARDIZATION & CLEANING (CRITICAL)\n",
    "# ==========================================\n",
    "def clean_fips(df):\n",
    "    \"\"\"\n",
    "    Standardizes the Federal Information Processing Standard (FIPS) codes\n",
    "    to ensure consistent merging keys across all datasets.\n",
    "    \"\"\"\n",
    "    # 1. Coerce to numeric to handle potential float representations (e.g., \"1001.0\")\n",
    "    df['fips'] = pd.to_numeric(df['fips'], errors='coerce')\n",
    "    \n",
    "    # 2. Remove observations where the FIPS code is missing or invalid\n",
    "    df = df.dropna(subset=['fips'])\n",
    "    \n",
    "    # 3. Convert to integer to eliminate decimal points (e.g., 1001.0 -> 1001)\n",
    "    df['fips'] = df['fips'].astype(int)\n",
    "    \n",
    "    # 4. Convert to string and apply zero-padding to ensure a fixed 5-digit format (e.g., 1001 -> \"01001\")\n",
    "    df['fips'] = df['fips'].astype(str).str.zfill(5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"--- Standardizing FIPS codes... ---\")\n",
    "df_votes = clean_fips(df_votes)\n",
    "df_eco   = clean_fips(df_eco)\n",
    "df_edu   = clean_fips(df_edu)\n",
    "df_demo  = clean_fips(df_demo)\n",
    "\n",
    "print(f\"‚úÖ Validation: Votes FIPS Example (Expected: 01001) : '{df_votes['fips'].iloc[0]}'\")\n",
    "print(f\"‚úÖ Validation: Eco FIPS Example   (Expected: 01001) : '{df_eco['fips'].iloc[0]}'\")\n",
    "\n",
    "# If both outputs confirm '01001' format, the subsequent merge operations will succeed.\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: DATA FUSION (MERGE)\n",
    "# ==========================================\n",
    "print(\"\\nMerging datasets...\")\n",
    "# Perform an INNER JOIN between Votes and Economics.\n",
    "# This intersection ensures we only retain counties present in both critical datasets.\n",
    "df_master = pd.merge(df_votes, df_eco, on='fips', how='inner') \n",
    "\n",
    "# Perform LEFT JOINS for Education and Demographics to preserve the main structure\n",
    "# even if minor auxiliary data is missing for some counties.\n",
    "df_master = pd.merge(df_master, df_edu, on='fips', how='left')\n",
    "df_master = pd.merge(df_master, df_demo, on='fips', how='left')\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: COLUMN CLEANING\n",
    "# ==========================================\n",
    "# Identify and remove redundant columns generated during the merge (suffixes like '_y').\n",
    "# Also remove duplicate identifier columns (e.g., 'county_name_eco').\n",
    "cols_to_drop = [c for c in df_master.columns if c.endswith('_y') or 'county_name_eco' in c]\n",
    "df_master = df_master.drop(columns=cols_to_drop)\n",
    "\n",
    "# Rename columns to remove the '_x' suffix generated by the merge.\n",
    "df_master.columns = df_master.columns.str.replace('_x', '')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: FINAL EXPORT (Master Table)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Configuration des chemins\n",
    "current_dir = Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# 2. S√©curit√© : Cr√©ation du dossier\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. Affichage des dimensions\n",
    "print(f\"\\nüìä Final Master Table Dimensions: {len(df_master)} rows.\")\n",
    "\n",
    "# 4. Logique de sauvegarde conditionnelle\n",
    "if len(df_master) > 0:\n",
    "    # D√©finition du chemin complet\n",
    "    output_filename = \"master_table_elections.csv\"\n",
    "    save_path_master = PROCESSED_DIR / output_filename\n",
    "    \n",
    "    # Export\n",
    "    df_master.to_csv(save_path_master, index=False)\n",
    "    \n",
    "    # Message de succ√®s avec le chemin pr√©cis\n",
    "    print(f\"üéâ SAVED! The master file has been populated here: {save_path_master}\")\n",
    "    print(df_master.head())\n",
    "else:\n",
    "    print(\"‚ùå ERROR: Dataset is empty. Deep investigation required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87a737",
   "metadata": {},
   "source": [
    "After having cleaned all of the individual dataset, it was time to merge all of those socio-demo-eco variables in a single dataset. This is the purpose of the code here above. We exported the merged dataset into our data/processed folder. This dataset is thus ready for ML analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
